{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the sentiment price prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run `$ bash setup.sh` to set up environments and download data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "!bash setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also set up the environment manually:\n",
    "!conda create --name sent_env --file requirements.\n",
    "\n",
    "# --or--\n",
    "# !conda conda env create -f environment.yml\n",
    "\n",
    "!mkdir -p ./data\n",
    "\n",
    "# then download the sql files:\n",
    "!gdown https://drive.google.com/file/d/1YG_AQIbcY2Mi-bKMLN1hff66jDBFeh4O/view?usp=sharing -O ./data/spx_news_sentiment_fundamental.db\n",
    "!gdown https://drive.google.com/file/d/1C49ElctSD0hPsukQTkioneA7PeWBlMfe/view?usp=sharing -O ./data/spx_news_sentiment_price.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "A dataset class has been implemented already. \n",
    "\n",
    "This uses the iterable style of the `Dataset` class.\n",
    "\n",
    "Initiate the BaseDataset class by passing a Config object, and wrap the kwargs\n",
    "into the Config object. \n",
    "\n",
    "When calling the `__iter__` method using `for data in dataset`, a triple will be returned `(x1, x2, y)`. x1 is the sentiment/price data, x2 is the past financial data up until the train data time, and y is the share price performance during the look_forward period (defaul 5 days, or a trading week). \n",
    "\n",
    "The `forward()` method needs to take in (x1, x2), as x2 carries information about the stock's fundamentals and type, but is of a different dimension as x1. An autoencoder model has been implemented and trained to convert the different dimensions of x2 into the same hidden dimensions (and is also useful for dimensionality reduction).\n",
    "\n",
    "Note x1 and x2 are scaled to between -1 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.datasets import BaseDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from utils import Config\n",
    "\n",
    "config = Config(mode='train', look_back=100, look_forward=5, num_workers=64)\n",
    "dataset = BaseDataset(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (LSTM_encoder): LSTM(100, 10)\n",
      "  (Conv1D_encoder): Conv1d(88, 88, kernel_size=(3,), stride=(1,))\n",
      "  (MaxPool1D_encoder): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Dense_encoder): Linear(in_features=352, out_features=5, bias=True)\n",
      "  (Dense_decoder): Linear(in_features=5, out_features=352, bias=True)\n",
      "  (MaxUnpool1D_decoder): MaxUnpool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (DeConv1D_decoder): ConvTranspose2d(88, 88, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (LSTM_decoder): LSTM(10, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.autoencoder import AutoEncoder\n",
    "model = AutoEncoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from utils import padding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "lr = 1e-4\n",
    "epoch = 50\n",
    "RUNNAME = f\"lr={lr}; epoch={epoch}\"\n",
    "\n",
    "writer = SummaryWriter(f'./logs/autoencoder/{RUNNAME}')\n",
    "\n",
    "input_dims = (88, 100)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.\n",
    "    for data in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        _, x2, _ = data\n",
    "        output, _ = model(x2)\n",
    "        output = padding(x2, direction='left', pad_value=0., \n",
    "            repeat=input_dims[-1] - output.shape[-1])\n",
    "        loss = criterion(output, x2)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    writer.add_scalar('loss', running_loss / len(dataset), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sent_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fa5fd258d5799b56102c839ebdb81d43440c6319b18894ac20aca9c2309d787"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
